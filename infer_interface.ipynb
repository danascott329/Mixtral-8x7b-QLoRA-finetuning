{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs_squad/merged_model\"\n",
    "# model_path = \"gmongaras/Wizard_7B_Squad_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "# Load in model\n",
    "def load_model(model_name):\n",
    "    # Load in the token\n",
    "    token = \"\"\n",
    "\n",
    "    # Load model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        use_auth_token=token,\n",
    "        do_sample=True,\n",
    "        cache_dir=\"models\",\n",
    "    ).eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        token=token\n",
    "    )\n",
    "    class Infer:\n",
    "        def __init__(self, model, tokenizer):\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        def forward(self, text, limit=128, temp=1.0):\n",
    "            text = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = self.model.generate(**text, do_sample=True, temperature=temp, max_new_tokens=int(limit), top_p=0.95, top_k=60, pad_token_id=self.tokenizer.pad_token_id)\n",
    "            return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return Infer(model, tokenizer)\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(temp, limit, text):\n",
    "    prompt = text\n",
    "    out = model.forward(prompt, limit, temp)\n",
    "    return out\n",
    "\n",
    "pred = gr.Interface(\n",
    "    predict,\n",
    "    inputs=[\n",
    "        gr.Slider(0.001, 10, value=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(1, 1024, value=128, label=\"Token Limit\"),\n",
    "        gr.Textbox(\n",
    "            label=\"Input\",\n",
    "            lines=1,\n",
    "            value=\"#### Human: What's the capital of Australia?#### Assistant: \",\n",
    "        ),\n",
    "    ],\n",
    "    outputs='text',\n",
    ")\n",
    "\n",
    "pred.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
